# 0x03. Optimization
## Specializations - Machine Learning â€• Supervised Learning
## Objectives
* What is a hyperparameter?
* How and why do you normalize your input data?
* What is a saddle point?
* What is stochastic gradient descent?
* What is mini-batch gradient descent?
* What is a moving average? How do you implement it?
* What is gradient descent with momentum? How do you implement it?
* What is RMSProp? How do you implement it?
* What is Adam optimization? How do you implement it?
* What is learning rate decay? How do you implement it?
* What is batch normalization? How do you implement it?

## Tasks
0. Normalization Constants

1. Normalize

2. Shuffle Data

3. Mini-Batch

4. Moving Average

5. Momentum

6. Momentum Upgraded

7. RMSProp

8. RMSProp Upgraded

9. Adam

10. Adam Upgraded

11. Learning Rate Decay

12. Learning Rate Decay Upgraded

13. Batch Normalization

14. Batch Normalization Upgraded

15. Put it all together and what do you get?

16. If you can't explain it simply, you don't understand it well enough
