# 0x11. Attention
## Specializations - Machine Learning â€• Supervised Learning
## Objectives
* What is the attention mechanism?
* How to apply attention to RNNs
* What is a transformer?
* How to create an encoder-decoder transformer model
* What is GPT?
* What is BERT?
* What is self-supervised learning?
* How to use BERT for specific NLP tasks
* What is SQuAD? GLUE?

## Requirements
* Your files will be executed with `numpy` (version 1.16) and `tensorflow` (version 1.15)
* Unless otherwise stated, you cannot import any module except `import tensorflow as tf`

## Tasks
0. RNN Encoder

1. Self Attention

2. RNN Decoder

3. Positional Encoding

4. Scaled Dot Product Attention

5. Multi Head Attention

6. Transformer Encoder Block

7. Transformer Decoder Block

8. Transformer Encoder

9. Transformer Decoder

10. Transformer Network
